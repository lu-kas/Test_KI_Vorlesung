[
  {
    "objectID": "notebooks/04-anwendungen.html",
    "href": "notebooks/04-anwendungen.html",
    "title": "04-anwendungen",
    "section": "",
    "text": "Platzhalter-Notebook f√ºr die Einheit.",
    "crumbs": [
      "Start",
      "04 ‚Äì Anwendungen"
    ]
  },
  {
    "objectID": "notebooks/01-einleitung.html",
    "href": "notebooks/01-einleitung.html",
    "title": "Einfaches neuronales Netzwerk ‚ÄúZu Fu√ü‚Äù",
    "section": "",
    "text": "Aufgabe: Implementierung eines Netzwerks, welches den Datensatz eines Polymoms ‚Äòlernt‚Äô.\n\\[\nf(x) = \\frac{1}{2}x^3 - 2x^2 + x\n\\]\nIm Wertebereich \\(x \\in [-2.5; 2.5]\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nDefinition der / des ‚Äòwahren‚Äô Funktion / Modells.\n\ndef true_fun(x):\n    return 0.5*x**3 - 2.0*x**2 + 1.0*x\n\nErzeugung von Datenpunkten anhand des wahren Modells, mit Rauschen (im Wertebereich).\n\nrng = np.random.default_rng(0)\nN = 300\nX = rng.uniform(-2.5, 2.5, size=(N, 1))\nY = true_fun(X) + rng.normal(0, 0.8, size=(N, 1))\n\n\nplt.scatter(X, Y, s=4, alpha=0.75)\nplt.xlabel(\"X, Eingangswert\")\nplt.ylabel(\"Y, Modellwert\")\nplt.grid()\n\n\n\n\n\n\n\n\nAufteilung in Trainings und Test Daten. Normierung der Werte.\n\n# Train/Test-Split\nperm = rng.permutation(N)\nsplit = int(0.8 * N)\nXtr, Ytr = X[perm[:split]], Y[perm[:split]]\nXte, Yte = X[perm[split:]], Y[perm[split:]]\n\n# Standardisierung (stabilisiert Training deutlich)\nx_mean = Xtr.mean(axis=0, keepdims=True)\nx_std  = Xtr.std(axis=0, keepdims=True) + 1e-8\ny_mean = Ytr.mean(axis=0, keepdims=True)\ny_std  = Ytr.std(axis=0, keepdims=True) + 1e-8\n\nXs_tr = (Xtr - x_mean) / x_std\nXs_te = (Xte - x_mean) / x_std\nYs_tr = (Ytr - y_mean) / y_std\nYs_te = (Yte - y_mean) / y_std\n\n\nplt.scatter(Xs_tr, Ys_tr, s=4, alpha=0.75, label=\"Trainingsdaten\")\nplt.scatter(Xs_te, Ys_te, s=4, alpha=0.75, label=\"Testdaten\")\nplt.xlabel(\"X, Eingangswert, normiert\")\nplt.ylabel(\"Y, Modellwert, normiert\")\nplt.legend()\nplt.grid()\n\n\n\n\n\n\n\n\nHilfsfunktionen. ToDo: Erl√§utern + plotten.\n\ndef tanh(z): \n    return np.tanh(z)\n\ndef dtanh(z):\n    t = np.tanh(z)\n    return 1.0 - t*t\n\ndef mse(yhat, y):\n    return ((yhat - y) ** 2).mean()\n\ndef mse_grad(yhat, y):\n    return 2.0 * (yhat - y) / y.shape[0]\n\nEine NN-Klasse um ein neuronales Netzwerk (Eingangslayer, 2 versteckte Layer, Ausgabelayer) zu definieren.\n\nclass NN:\n    def __init__(self, in_dim=1, h1=20, h2=20, seed=1):\n        rng = np.random.default_rng(seed)\n        # Glorot-Normal (gut f√ºr tanh)\n        def glorot(fan_in, fan_out):\n            std = np.sqrt(2.0 / (fan_in + fan_out))\n            return rng.normal(0.0, std, size=(fan_in, fan_out))\n\n        self.W1 = glorot(in_dim, h1); self.b1 = np.zeros((1, h1))\n        self.W2 = glorot(h1, h2);     self.b2 = np.zeros((1, h2))\n        self.W3 = glorot(h2, 1);      self.b3 = np.zeros((1, 1))\n\n    # Netzwerk forw√§rts durchlaufen\n    def forward(self, X):\n        z1 = X @ self.W1 + self.b1\n        a1 = tanh(z1)\n        z2 = a1 @ self.W2 + self.b2\n        a2 = tanh(z2)\n        yhat = a2 @ self.W3 + self.b3\n        cache = (X, z1, a1, z2, a2, yhat)\n        return yhat, cache\n\n    # Netzwerk r√ºckw√§rts durchlaufen (f√ºrs Training)\n    def backward(self, cache, yhat, y, l2=0.0):\n        X, z1, a1, z2, a2, _ = cache\n\n        dy = mse_grad(yhat, y)\n\n        # Layer 3\n        dW3 = a2.T @ dy\n        db3 = dy.sum(axis=0, keepdims=True)\n\n        # Layer 2\n        da2 = dy @ self.W3.T\n        dz2 = da2 * dtanh(z2)\n        dW2 = a1.T @ dz2\n        db2 = dz2.sum(axis=0, keepdims=True)\n\n        # Layer 1\n        da1 = dz2 @ self.W2.T\n        dz1 = da1 * dtanh(z1)\n        dW1 = X.T @ dz1\n        db1 = dz1.sum(axis=0, keepdims=True)\n\n        # L2 (optional) auf Gewichte\n        if l2 &gt; 0.0:\n            dW3 += l2 * self.W3\n            dW2 += l2 * self.W2\n            dW1 += l2 * self.W1\n\n        grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2, \"W3\": dW3, \"b3\": db3}\n        return grads\n\n    # Update der Gewichte anhand eines Gradienten und einer Lernrate\n    def step(self, grads, lr):\n        self.W1 -= lr * grads[\"W1\"]; self.b1 -= lr * grads[\"b1\"]\n        self.W2 -= lr * grads[\"W2\"]; self.b2 -= lr * grads[\"b2\"]\n        self.W3 -= lr * grads[\"W3\"]; self.b3 -= lr * grads[\"b3\"]\n\nToDo: einfache Visualisierung des Netzwerks m√∂glich? Ggf. was von ChatGPT programmieren lassen üòÅ\nTraining des Netzes.\n\nmodel = NN(in_dim=1, h1=20, h2=20, seed=1)\nepochs = 800\nlr = 0.03\nbatch = 64\nl2 = 0.0  # z.B. 1e-4 f√ºr leichte Gl√§ttung\n\ntrain_hist = []\ntest_hist = []\n\nfor ep in range(1, epochs + 1):\n    idx = rng.permutation(len(Xs_tr))\n    for i in range(0, len(idx), batch):\n        b = idx[i:i+batch]\n        xb, yb = Xs_tr[b], Ys_tr[b]\n\n        yhat, cache = model.forward(xb)\n        grads = model.backward(cache, yhat, yb, l2=l2)\n        model.step(grads, lr)\n\n    # Tracking (auf skalierten Gr√∂√üen)\n    yhat_tr, _ = model.forward(Xs_tr)\n    yhat_te, _ = model.forward(Xs_te)\n    train_hist.append(mse(yhat_tr, Ys_tr))\n    test_hist.append(mse(yhat_te, Ys_te))\n\n    if ep % 100 == 0 or ep == 1:\n        print(f\"Epoch {ep:3d} | MSE train {train_hist[-1]:.4f} | MSE test {test_hist[-1]:.4f}\")\n\nEpoch   1 | MSE train 0.6644 | MSE test 0.8430\nEpoch 100 | MSE train 0.0541 | MSE test 0.0768\nEpoch 200 | MSE train 0.0291 | MSE test 0.0361\nEpoch 300 | MSE train 0.0250 | MSE test 0.0289\nEpoch 400 | MSE train 0.0232 | MSE test 0.0248\nEpoch 500 | MSE train 0.0218 | MSE test 0.0234\nEpoch 600 | MSE train 0.0213 | MSE test 0.0222\nEpoch 700 | MSE train 0.0210 | MSE test 0.0208\nEpoch 800 | MSE train 0.0207 | MSE test 0.0208\n\n\n\n# Denormierte Vorhersagen auf Test\nyhat_te_dn = yhat_te * y_std + y_mean\nmse_test_dn = mse(yhat_te_dn, Yte)\nss_res = float(((yhat_te_dn - Yte) ** 2).sum())\nss_tot = float(((Yte - Yte.mean()) ** 2).sum())\nr2 = 1.0 - ss_res / ss_tot\nprint(f\"Denormierte Test-MSE: {mse_test_dn:.4f} | R^2: {r2:.3f}\")\n\nDenormierte Test-MSE: 0.6466 | R^2: 0.983\n\n\n\n# Fit-Kurve gegen wahres Modell und Datenpunkte\nxgrid = np.linspace(X.min()-1.3, X.max()+1.3, 400, dtype=np.float32).reshape(-1, 1)\nxgrid_s = (xgrid - x_mean) / x_std\nygrid_s, _ = model.forward(xgrid_s)\nygrid = ygrid_s * y_std + y_mean\n\nplt.figure(figsize=(7, 5))\nplt.scatter(Xtr, Ytr, s=15, alpha=0.5, label=\"Trainingdaten\")\nplt.scatter(Xte, Yte, s=15, alpha=0.7, label=\"Testdaten\")\nplt.plot(xgrid, true_fun(xgrid), linewidth=2, label=\"Wahre Funktion f(x)\")\nplt.plot(xgrid, ygrid, linewidth=2, linestyle=\"--\", label=\"NN Vorhersage\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.grid()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# Lernkurven (MSE auf skalierten Targets)\nplt.figure(figsize=(7, 4))\nplt.plot(train_hist, label=\"Trainingsdaten\")\nplt.plot(test_hist, label=\"Testdaten\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE (skaliert)\")\nplt.legend()\nplt.tight_layout()\nplt.grid()\n\n\n\n\n\n\n\n\n\n# Lernkurven (MSE auf skalierten Targets)\nplt.figure(figsize=(7, 4))\nplt.plot(train_hist, label=\"Trainingsdaten\")\nplt.plot(test_hist, label=\"Testdaten\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE (skaliert)\")\nplt.legend()\nplt.tight_layout()\nplt.grid()\nplt.yscale('log')",
    "crumbs": [
      "Start",
      "01 ‚Äì Einf√ºhrung"
    ]
  },
  {
    "objectID": "notebooks/03-methoden.html",
    "href": "notebooks/03-methoden.html",
    "title": "03-methoden",
    "section": "",
    "text": "Platzhalter-Notebook f√ºr die Einheit.",
    "crumbs": [
      "Start",
      "03 ‚Äì Methoden"
    ]
  },
  {
    "objectID": "includes/callout-tip.html",
    "href": "includes/callout-tip.html",
    "title": "Vorlesung KI @ BauIng",
    "section": "",
    "text": "Tip\n\n\n\nDies ist ein Tipp-Block, den du wiederverwenden kannst."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vorlesung KI @ BauIng",
    "section": "",
    "text": "Diese Website b√ºndelt die Inhalte der Vorlesung in 6 Einheiten. Jede Einheit ist als Jupyter-Notebook eingebunden.\n\n\n\n\n\n\nTip\n\n\n\nNutze die Suche in der linken Sidebar, um schnell zu Themen zu springen.\n\n\n\n\n\nInteraktive Codebl√∂cke aus den Notebooks\nAbbildungen und Tabellen mit automatischer Nummerierung\nLiteraturverwaltung √ºber references.bib\n\n\n\n\n\nEinf√ºhrung\n\nGrundlagen\n\nMethoden\n\nAnwendungen\n\nFallstudie\n\nZusammenfassung",
    "crumbs": [
      "Start",
      "Vorlesung KI @ BauIng"
    ]
  },
  {
    "objectID": "index.html#aufbau",
    "href": "index.html#aufbau",
    "title": "Vorlesung KI @ BauIng",
    "section": "",
    "text": "Interaktive Codebl√∂cke aus den Notebooks\nAbbildungen und Tabellen mit automatischer Nummerierung\nLiteraturverwaltung √ºber references.bib",
    "crumbs": [
      "Start",
      "Vorlesung KI @ BauIng"
    ]
  },
  {
    "objectID": "index.html#einheiten",
    "href": "index.html#einheiten",
    "title": "Vorlesung KI @ BauIng",
    "section": "",
    "text": "Einf√ºhrung\n\nGrundlagen\n\nMethoden\n\nAnwendungen\n\nFallstudie\n\nZusammenfassung",
    "crumbs": [
      "Start",
      "Vorlesung KI @ BauIng"
    ]
  },
  {
    "objectID": "notebooks/02-grundlagen.html",
    "href": "notebooks/02-grundlagen.html",
    "title": "02-grundlagen",
    "section": "",
    "text": "Platzhalter-Notebook f√ºr die Einheit.",
    "crumbs": [
      "Start",
      "02 ‚Äì Grundlagen"
    ]
  },
  {
    "objectID": "notebooks/05-fallstudie.html",
    "href": "notebooks/05-fallstudie.html",
    "title": "05-fallstudie",
    "section": "",
    "text": "Platzhalter-Notebook f√ºr die Einheit.",
    "crumbs": [
      "Start",
      "05 ‚Äì Fallstudie"
    ]
  },
  {
    "objectID": "notebooks/06-zusammenfassung.html",
    "href": "notebooks/06-zusammenfassung.html",
    "title": "06-zusammenfassung",
    "section": "",
    "text": "Platzhalter-Notebook f√ºr die Einheit.",
    "crumbs": [
      "Start",
      "06 ‚Äì Zusammenfassung"
    ]
  }
]